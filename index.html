<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ramya Hebbalaguppe</title>
  
  <meta name="author" content="Ramya Hebbalaguppe">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ramya Hebbalaguppe</name>
              </p>
              <p> I'm a researcher in visual computing. My work spans: computer vision, reliable and robust machine learning, and computer graphics. I'm a Principal Scientist in Deep Learning and Artificial Intelligence Group (DLAI), TCS Research, IIT Delhi, India. My doctoral research advised by <a href="https://www.cse.iitd.ac.in/~chetan/">Prof. Chetan Arora</a> at IIT Delhi is centered on proposing novel methods to enhance reliability in deep neural networks (DNNs)
                </p>

                <p>  Prior to this, I was fortunate to be working with <a href="https://sites.google.com/site/ramakrishnakakarala/research?authuser=0">Prof. Ramakrishna Kakarala</a> at Nanyang Technological University on High Dynamic Range Imaging algorithms which formed a part of the image processing pipeline aimed at smartphone cameras.  Our research was recognized with the Best Student Paper award at the 2012 SPIE conference in Burlingame, California. I completed my master's degree at DCUs School of Electronic Engineering and Computing in 2014, advised by <a href="https://www.insight-centre.org/our-team/prof-noel-oconnor/">Prof. Noel O'Connor</a> and <a href="https://www.dcu.ie/insight/people/alan-smeaton">Prof. Alan Smeaton</a>. I focused on reducing false alarms in surveillance camera networks. The result of this work, a portion of our research was licensed to Netwatch Systems.
              </p>
              <p>
                In June 2015, I started working as a research scientist at TCS Research. Since the, I have been involved in various projects related to augmented reality. Specifically, I have focused on optimizing the layout of labels for immersive experiences and developing gestural interfaces for head-mounted devices and smartphones. As a team leader, I have overseen the development of a cost-effective industrial inspection framework. Recently, my team has been working on creative content generation (images, videos, 3D/4D data).</a>
              </p>

              <p>
                Outside work, I enjoy painting, traveling, cooking and baking, composting, planting tree saplings, and music.</a>
              </p>
              
              <p style="text-align:center">>
                <a href="mailto:ramya.murthy@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=IJjnjZIAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/RHebbalaguppe">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/rhebbalaguppe">Github</a> &nbsp/&nbsp
                <a href="travel.html">Travel</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/playa.png"><img style="width:120%;max-width:120%" alt="profile photo" src="images/playa.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                Representative papers spanning the following themes reliable machine learning (out-of-distribution detection, uncertainty quantification, continual learning), 3D/4D/2D computer vision are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
      

          <!-- Add new papers here -->

                    <!-- LOMOE  -->

          
            <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='images/StyleCL.png' width="225" height ="175">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle>Lifelong Learning in StyleGAN through Latent Subspaces
</papertitle>
              </a>
              <br>
              <a href="https://dblp.org/pid/255/5908.html">Adarsh K</a>,
              <a> Anmol Garg </a>,
              <strong>Ramya Hebbalaguppe</strong>,
              <a href="https://eecs.iisc.ac.in/people/prathosh-a-p/">Prathosh AP</a>,
              <br>
              <em>Transactions on Machine Learning Research (accepted) </em>, 2024 &nbsp  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <br>
              <!-- <a href="https://arxiv.org/abs/2403.00437">[paper]</a> -->
              <!-- / <a href="">arXiv</a> -->
              <!-- / <a href="">Additional</a> -->
              <p></p>
              <p>
              StyleGAN is one of the most versatile generative models that have emerged in recent times. However, when it is trained continually on a stream of data (potentially previously unseen distributions), it tends to forget the distribution it has learned, as is the case with any other generative model, due to catastrophic forgetting. Recent studies have shown that the latent space of StyleGAN is very versatile, as data from a variety of distributions can be inverted onto it. In this paper, we propose StyleCL, a method that leverages this property to enable lifelong learning in StyleGAN without forgetting. Specifically, given a StyleGAN trained on a certain task (dataset), we propose to learn a latent subspace characterized by a set of dictionary vectors in its latent space, one for each novel, unseen task (or dataset). We also learn a relatively small set of parameters (feature adaptors) in the weight space to complement the dictionary learning in the latent space. Furthermore, we introduce a method that utilizes the similarity between tasks to effectively reuse the feature adaptor parameters from the previous tasks, aiding in the learning process for the current task at hand. Our approach guarantees that the parameters from previous tasks are reused only if they contribute to a beneficial forward transfer of knowledge. Remarkably, StyleCL avoids catastrophic forgetting because the set of dictionary and the feature adaptor parameters are unique for each task. We demonstrate that our method, StyleCL, achieves better generation quality on multiple datasets with significantly fewer additional parameters per task compared to previous methods. This is a consequence of learning task-specific dictionaries in the latent space, which has a much lower dimensionality compared to the weight space.
              </p>
            </td>
          </tr>


          <!-- KD(C)  -->

          
            <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='images/Teaser_accv.png' width="200" height ="150">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle>Calibration Transfer via Knowledge Distillation
</papertitle>
              </a>
              <br>
              <strong>Ramya Hebbalaguppe</strong>,
              <a href="https://www.sc.iitb.ac.in/~mayank/">Mayank Baranwal</a>,
              <a> Kartik Anand</a>,
              <a href="https://www.cse.iitd.ac.in/~chetan/">Chetan Arora</a>,
              <br>
              <em>ACCV </em>, 2024 &nbsp -- <font color="red"><strong>[Oral Presentation] (top 5.6 &#37)</strong></font>
              <br>
              <a>[paper]</a>
              <!-- / <a href="">arXiv</a> -->
              <!-- / <a href="">Additional</a> -->
              <p></p>
              <p>
 Knowledge Distillation for Calibration (</strong>KD(C)</strong>) endeavors to deploy <strong>lightweight</strong> models that are also <strong>reliable</strong>, we delve into the realm of knowledge distillation, extending its traditional function of transferring accuracy from teacher networks to student networks. Through this exploration, we have discovered a novel approach to calibrating models effectively. We present, arguably for the first time, compelling theoretical as well as empirical evidence that model calibration can be achieved without sacrificing accuracy through knowledge distillation. Our implementation of knowledge distillation not only guarantees enhanced model calibration but also outperforms the accuracy obtained through conventional training from scratch in specific cases. This innovative approach enables us to simultaneously accomplish the dual objectives of optimal calibration and improved accuracy.
              </p>
            </td>
          </tr>


          <!-- LOMOE  -->

          
            <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='images/LOMOE_teaser_2.png' width="200" height ="150">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle>LoMOE: Localized Multi-Object Editing via Multi-Diffusion
</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.co.in/citations?user=jxW0icUAAAAJ&hl=en/">Goirik Chakrabarty</a>,
              <a href="https://scholar.google.com/citations?user=VN9KgoUAAAAJ&hl=en/">Aditya Chandrasekar</a>,
              <strong>Ramya Hebbalaguppe</strong>,
              <a href="https://eecs.iisc.ac.in/people/prathosh-a-p/">Prathosh AP</a>,
              <br>
              <em>ACM International Conference on Multimedia </em>, 2024 &nbsp  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <br>
              <a href="https://arxiv.org/abs/2403.00437">[paper]</a>
              <!-- / <a href="">arXiv</a> -->
              <!-- / <a href="">Additional</a> -->
              <p></p>
              <p>
              Recent developments in the field of diffusion models have demonstrated an exceptional capacity to generate high-quality prompt-conditioned image edits. Nevertheless, previous approaches have primarily relied on textual prompts for image editing, which tend to be less effective when making precise edits to specific objects or fine-grained regions within a scene containing single/multiple objects. We introduce a novel framework for zero-shot localized multi-object editing through a multi-diffusion process to overcome this challenge. This framework empowers users to perform various operations on objects within an image, such as adding, replacing, or editing many objects in a complex scene in one pass. We also curate and release a dataset dedicated to multi-object editing, named LoMOE-Bench. Our experiments against existing state-of-the-art methods demonstrate the improved effectiveness of our approach in terms of both image editing quality and inference speed.
              </p>
            </td>
          </tr>
      
          <!--\ ReMOVE  -->

<!-- ReMOVE  -->

          <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()">
          <!-- <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()" bgcolor="#ffffd0">-->  <!-- For oral papers uncomment this line -->
            <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='images/ReMOVE_Teaser.png' width="200" height ="130">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle>ReMOVE: A Reference-free Metric for Object Erasure</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=VN9KgoUAAAAJ&hl=en/">Aditya Chandrasekar</a>,
              <a href="https://scholar.google.co.in/citations?user=jxW0icUAAAAJ&hl=en/">Goirik Chakrabarty</a>,
              <a href="https://in.linkedin.com/in/jai-bardhan/">Jai Bardhan</a>,
              <strong>Ramya Hebbalaguppe</strong>,
              <a href="https://eecs.iisc.ac.in/people/prathosh-a-p/">Prathosh AP</a>,
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPRW) The First Workshop on the Evaluation of Generative Foundation Models</em>, 2024 &nbsp  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <br>
              <a href="https://github.com/chandrasekaraditya/ReMOVE">project page</a>
              <!-- / <a href="">arXiv</a> -->
              <!-- / <a href="">Additional</a> -->
              <p></p>
              <p>
              We introduce ReMOVE, a novel reference-free metric for assessing object erasure efficacy in diffusion-based image editing models post-generation. Unlike existing measures such as LPIPS and CLIPScore, ReMOVE addresses the challenge of evaluating inpainting without a reference image, common in practical scenarios. ReMOVE  effectively distinguishes between object removal and replacement, a key issue in diffusion models due to stochastic nature of image generation. 
              </p>
            </td>
          </tr>
      
          <!--\ ReMOVE  -->


          <!-- Transfer4D  -->

          <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()">
          <!-- <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()" bgcolor="#ffffd0">-->  <!-- For oral papers uncomment this line -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='transfer4D_image'><video  width=100%  muted autoplay loop> <!-- Note:- Make sure to change `transfer4D_image` for the new paper -->
                <source src="images/Transfer4D-CVPR-cropped.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/Transfer4D-CVPR-cropped.jpeg' width="100" height ="200">
              </div>
              <script type="text/javascript">
                function dreamfusion_start() {
                  document.getElementById('transfer4D_image').style.opacity = "1"; // Note:- Make sure to change `transfer4D_image` for the new paper
                }

                function dreamfusion_stop() {
                  document.getElementById('transfer4D_image').style.opacity = "0"; // Note:- Make sure to change `transfer4D_image` for the new paper
                }
                dreamfusion_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dreamfusion3d.github.io/">
                <papertitle>Transfer4D: A framework for frugal motion capture and deformation transfer</papertitle>
              </a>
              <br>
              <a href="https://shubhmaheshwari.github.io/website/">Shubh Maheshwari</a>,
              <a href="https://www.cse.iitd.ac.in/~narain/">Rahul Narain</a>,
              <strong>Ramya Hebbalaguppe</strong>,
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 &nbsp  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <br>
              <a href="https://transfer4d.github.io/#">project page</a>
              <!-- / <a href="">arXiv</a> -->
              <!-- / <a href="">Additional</a> -->
              <p></p>
              <p>
              Animating a virtual character based on a real performance of an actor is a challenging task that currently requires expensive motion capture setups and additional effort by expert animators, rendering it accessible only to large production houses. The goal of our work is to democratize this task by developing a frugal alternative termed Transfer4D that uses only commodity depth sensors and further reduces animators' effort by automating the rigging and animation transfer process. Our approach can transfer motion from an incomplete, single-view depth video to a semantically similar target mesh, unlike prior works that make a stricter assumption on the source to be noise-free and watertight.
              </p>
            </td>
          </tr>
		  
          <!--\ Transfer4D  -->


          <!-- Calibration on data-diet  -->

          <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()">
           <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/data_diet.png' width="160">
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/WACV2023/html/Patra_Calibrating_Deep_Neural_Networks_Using_Explicit_Regularisation_and_Dynamic_Data_WACV_2023_paper.html">
                <papertitle>Calibrating Deep Neural Networks Using Explicit Regularisation and Dynamic Data Pruning</papertitle>
              </a>
              <br>
              <a href="https://hades-rp2010.github.io/">Rishabh Patra*</a>,
              <strong>Ramya Hebbalaguppe*</strong>,
              <a href="https://tirtharajdash.github.io/">Tirtharaj Dash</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=f70Rc2wAAAAJ">Gautam Shroff</a>,
              <a href="https://scholar.google.com/citations?user=M5MIeROF8JYC&hl=en">Lovekesh Vig</a>,
              <br>
              <em>IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2023  &nbsp  -- <font color="red"><strong>[Spotlight Presentation] (top 10&#37)</strong></font> 
              <br>
              <a href="https://openaccess.thecvf.com/content/WACV2023/html/Patra_Calibrating_Deep_Neural_Networks_Using_Explicit_Regularisation_and_Dynamic_Data_WACV_2023_paper.html/#">project page</a>
              <!-- / <a href="">arXiv</a> -->
              <!-- / <a href="">Additional</a> -->
              <p></p>
              <p>
              We demonstrate state-of-the-art Deep Neural Network calibration performance via proposing a differentiable loss term
that can be used effectively in gradient descent optimisation and dynamic data pruning strategy not only enhances legitimate
high confidence samples to enhance trust in DNN classifiers but also reduce the training time for calibration.
              </p>
            </td>
          </tr>
      
<!-- CnC -->

          <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()">
           <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/CnC.png' width="160">
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/cnc-ood/">
                <papertitle>A Novel Data Augmentation Technique for Out-of-Distribution Sample Detection using Compounded Corruptions</papertitle>
              </a>
              <br>
              <strong>Ramya Hebbalaguppe</strong>,
              <a href="https://soumya1612-rasha.github.io/Soumya/">Soumya Suvra Ghosal</a>,
              <a href="https://in.linkedin.com/in/jatin-prakash-6874b91a4">Jatin Prakash</a>,
              <a href="https://sites.google.com/view/harshad/home">Harshad Khadilkar</a>,
              <a href="https://www.cse.iitd.ac.in/~chetan/">Chetan Arora</a>,
              
              <br>
              <em>European Conference on Machine Learning </em>, 2022 &nbsp  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <br>
              <a href="https://github.com/cnc-ood/#">project page</a>
              <!-- / <a href="">arXiv</a> -->
              <!-- / <a href="">Additional</a> -->
              <p></p>
              <p>
              We propose a novel Compounded Corruption(CnC) technique for the Out-of-Distribution data augmentation. One of the major advantages of CnC is that it does not require any hold-out data apart from the training set. Our extensive comparison with 20 methods from the major conferences in last 4 years show that a model trained using CnC based data augmentation, significantly outperforms SOTA, both in terms of OOD detection accuracy as well as inference time.
              </p>
            </td>
          </tr>
      
          <!--\ Transfer4D  -->


                    <!-- MDCA -->

          <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()">
           <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/stitch_in_time.png' width="160">
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/mdca-loss/MDCA-Calibration">
                <papertitle> A stitch in time saves nine: A train-time regularizing loss for improved neural network calibration </papertitle>
              </a>
              <br>
              <strong>Ramya Hebbalaguppe*</strong>,
              <a href="https://in.linkedin.com/in/jatin-prakash-6874b91a4">Jatin Prakash</a>,
              <strong>Neelabh Madan*</strong>,
              <a href="https://www.cse.iitd.ac.in/~chetan/">Chetan Arora</a>,
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2022  &nbsp  -- <font color="red"><strong>[ORAL Presentation] (top 4&#37)</strong></font> 
              <br>
              <a href="https://github.com/mdca-loss/MDCA-Calibration/#">project page</a>
              <!-- / <a href="">arXiv</a> -->
              <!-- / <a href="">Additional</a> -->
              <p></p>
              <p>
              We propose a novel auxiliary loss function: Multi-class Difference in Confidence and Accuracy (MDCA) for Deep Neural Network calibration. The loss can be combined with any application specific classification losses for image, NLP, Speech domains. We also demonstrate the utility of the loss in semantic segmentation tasks.
              </p>
            </td>
          </tr>
      
          <!--\ MDCA  -->


        </tbody></table>

        <section width="100%" align="left" border="0" cellspacing="0" cellpadding="20">
          <h2>Current Research Team, Research and Innovation Park, IIT Delhi</h2>
          <!-- <ul style="list-style-type: none;">   -->
          <ol>
            <!--<li><a style="text-decoration: none;" href="https://www.linkedin.com/in/tamoghno-kandar/?originalSubdomain=in"> Tamoghno Kandar (IIT Bombay) <strong>Topic:</strong>  Enhancing Trustworthiness in Foundational Models  </a></li> -->
            <li><a style="text-decoration: none;" href="https://researchweb.iiit.ac.in/~jai.bardhan/"> Jai Bardhan (IIIT Hyderabad) <strong>Topic:</strong>  3D Skeletonization/ Diffusion Models for creative content creation </a></li>
            
               
            
            

          </ol>
        </section>


        <section width="100%" align="left" border="0" cellspacing="0" cellpadding="20">
          <h2> Alumni </h2>
          <h3> (The list of researchers include full-time, pre-doctoral fellows, and research interns.)</h3>
          <ol>     

            <li><a style="text-decoration: none;" > Adarsh Kappiyath &#8594 Doctoral student at the University of Surrey, UK
            <li><a style="text-decoration: none;" > Meghal Dani &#8594 Doctoral student at IMPRS-IS, Max Planck School </a></li>
            <li><a style="text-decoration: none;"href="https://surabhisnath.github.io"> Surabhi Nath &#8594 Doctoral student at the Max Planck School of Cognition and the MPI for Biological Cybernetics </a></li>   
            <li><a style="text-decoration: none;" >  Goirik Chakrabarty &#8594 Doctoral student, University G√∂ttingen </a></li>
            <li><a style="text-decoration: none;" href="https://in.linkedin.com/in/jatin-prakash-6874b91a4"> Jatin Prakash &#8594 Doctoral student at New York University</a></li>
            <li><a style="text-decoration: none;" > Neelabh Madan &#8594 Doctoral student at New York University</a></li>
            <li><a style="text-decoration: none;" href="https://gaurav16gupta.github.io/"> Gaurav Gupta &#8594 Doctoral student at Rice University</a></li>
            <li><a style="text-decoration: none;" href="https://research.manchester.ac.uk/en/persons/apoorv.khattar-postgrad"> Apoorv Khattar &#8594 Doctoral student at University of Manchester, UK</a></li>
            <li><a style="text-decoration: none;" > Neel Rakholia &#8594 Masters Student at Stanford</a></li>
            <li><a style="text-decoration: none;" href="https://sharanry.github.io/">  Sharan Yalburgi &#8594 Visiting researcher at MIT proabilistic ML project</a></li>
            <li><a style="text-decoration: none;" href="https://srihegde.github.io/"> Srinidhi Hegde &#8594 Masters Student at UMD</a></li>
            <li><a style="text-decoration: none;" href="https://shubhmaheshwari.github.io/"> Shubh Maheshwari &#8594 Graduate student at UCSD</a></li>

            <li><a style="text-decoration: none;" > Pranay Gupta &#8594 Masters student at CMU</a></li>
            <li><a style="text-decoration: none;" > Jitender Maurya &#8594 Researcher, Toshiba</a></li>
            <li><a style="text-decoration: none;" > Archie Gupta &#8594 SDE, Microsoft</a></li>
            <li><a style="text-decoration: none;" > Varun Jain &#8594 Masters student at CMU &#8594 Microsoft Fellow</a></li>
            <li><a style="text-decoration: none;" href="https://www.linkedin.com/in/addityapopli/?originalSubdomain=in"> Additya Popli &#8594 SDE at Google</a></li>
            <li><a style="text-decoration: none;" > Kshitijz Jain &#8594 Grad student at IITD </a></li>
            <li><a style="text-decoration: none;" >  Aravind Udupa &#8594 Grad student at IITD </a></li>
            
            
            <li><a style="text-decoration: none;" > Soumya Suvra Ghosal &#8594 Masters Student at University of Wisconsin</a></li>
            <li><a style="text-decoration: none;" > Gaurav Garg &#8594  Accenture</a></li>
            
            <li><a style="text-decoration: none;" > Ramakrishna Perla &#8594 TTEC Digital</a></li>

          </ol>
        </section>


          <section width="100%" align="left" border="0" cellspacing="0" cellpadding="20">
          <h2>Academia: Thesis supervision</h2>
          <ol>
            <li><a style="text-decoration: none;" > Aditya C (IISc, Bangalore) - co-supervisor for M. Tech Thesis <strong>Topic:</strong> Metrics for image editing</a></li> 
            <li><a style="text-decoration: none;" href="https://www.linkedin.com/in/shreyashmohatta/"> Shreyash Mohatta (BITS, Goa) - supervised M.Tech thesis on Egocentric Realtime Gesture Recognition with Dr. Ashwin Srinivasan &#8594 Masters student at NCSU</a></li>
            <li><a style="text-decoration: none;" > Rishabh Patra  (BITS, Goa)- supervised B.Tech thesis on uncertainty calibration with Dr. Tirtharaj Dash &#8594 SDE Amazon </a></li>
            <li><a style="text-decoration: none;" href="https://sites.google.com/view/ashwinvaswani"> Ashwin Vaswani (BITS, Goa) - supervised BTP on Data-free Iterative Knowledge Distillation with Prof. Ashwin Srinivasan &#8594 Google Research &#8594 Masters student at CMU</a></li>
            <li><a style="text-decoration: none;" href="https://scholar.google.com/citations?user=ZdTC1joAAAAJ&hl=en"> Het Shah
 (BITS, Goa) - supervised BTP on Knowledge Distillation,
Pruning and Quantization with Prof. Ashwin Srinivasan &#8594 Research Associate at Google Research</a></li>
        </ol>
        </section>




				<!-- Uncomment to add misc section -->
        <!--, such as confidence calibration, detecting out-of-distribution sample to abstain from making decisions on unknown classes, and improving generalization in a continual learning setting. Additionally, my work also has been in optimizing DNN parameters to obtain reliable light weight models. -->
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody> -->
					
        <!-- Uncomment to Area chair info -->
        <!-- 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr> -->

				<!-- Uncomment to add blog post info	 -->
        <!--
          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Basically <br> Blog Posts</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
              <br>
              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
              <br>
              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
            </td>
          </tr> -->
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Website inspired from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
